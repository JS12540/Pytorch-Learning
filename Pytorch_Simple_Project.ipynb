{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qg-EJ8isMUe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b7eda9-5969-420c-8eb9-48108e42d774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[0.0136]], grad_fn=<AddmmBackward0>)\n",
            "Loss: tensor(0.8381, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch  # Imports the torch library\n",
        "import torch.nn as nn  # Imports the neural network module from PyTorch\n",
        "import torch.optim as optim  # Imports the optimization module from PyTorch\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):  # Defines a class named SimpleNN inheriting from nn.Module\n",
        "    def __init__(self, input_size, hidden_size, output_size):  # Initializes the class with input, hidden, and output sizes\n",
        "        super(SimpleNN, self).__init__()  # Calls the constructor of the parent class\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Defines the first fully connected layer\n",
        "        self.relu = nn.ReLU()  # Defines the ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)  # Defines the second fully connected layer\n",
        "\n",
        "    def forward(self, x):  # Defines the forward pass of the neural network\n",
        "        out = self.fc1(x)  # Performs the first linear transformation\n",
        "        out = self.relu(out)  # Applies the ReLU activation function\n",
        "        out = self.fc2(out)  # Performs the second linear transformation\n",
        "        return out  # Returns the output\n",
        "\n",
        "# Initialize weights and biases\n",
        "def init_weights(m):  # Defines a function to initialize weights and biases\n",
        "    if isinstance(m, nn.Linear):  # Checks if the module is an instance of nn.Linear\n",
        "        nn.init.normal_(m.weight, mean=0, std=0.1)  # Initializes weights from a normal distribution\n",
        "        nn.init.constant_(m.bias, 0)  # Initializes biases to zero\n",
        "\n",
        "# Create an instance of the network\n",
        "input_size = 10  # Specifies the size of the input\n",
        "hidden_size = 20  # Specifies the size of the hidden layer\n",
        "output_size = 1  # Specifies the size of the output\n",
        "model = SimpleNN(input_size, hidden_size, output_size)  # Creates an instance of the neural network\n",
        "\n",
        "# Initialize the weights and biases of the model\n",
        "model.apply(init_weights)  # Applies weight initialization to the model\n",
        "\n",
        "# Define a sample input\n",
        "x = torch.randn(1, input_size)  # Generates random input data\n",
        "\n",
        "# Perform a forward pass\n",
        "output = model(x)  # Performs a forward pass through the model\n",
        "print(\"Output:\", output)  # Prints the output of the model\n",
        "\n",
        "# Define a sample target\n",
        "target = torch.randn(1, output_size)  # Generates random target data\n",
        "\n",
        "# Calculate loss (for demonstration purposes, you can use any loss function)\n",
        "criterion = nn.MSELoss()  # Defines the Mean Squared Error loss function\n",
        "loss = criterion(output, target)  # Calculates the loss between the output and target\n",
        "print(\"Loss:\", loss)  # Prints the loss value\n",
        "\n",
        "# Perform backward pass and update gradients\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Defines the optimizer with Stochastic Gradient Descent\n",
        "optimizer.zero_grad()  # Clears the gradients of all optimized tensors\n",
        "loss.backward()  # Computes gradients of loss w.r.t. model parameters\n",
        "optimizer.step()  # Updates the model parameters based on gradients and optimizer settings\n"
      ]
    }
  ]
}